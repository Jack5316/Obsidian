---
type: arxiv-digest
date: 2026-02-20
topics: [large language model, transformer architecture, VLSI design, integrated circuit, philosophy of mind]
paper_count: 26
tags:
  - source/arxiv
  - research
---

# ArXiv Digest - 2026-02-20

> [!info] 26 papers across 5 topics (last 7 days)

## Papers

| Title | Authors | Date | Categories |
|-------|---------|------|------------|
| [What Language is This? Ask Your Tokenizer](https://arxiv.org/abs/2602.17655v1) | Clara Meister, Ahmetcan Yavuz et al. | 2026-02-19 | cs.CL |
| [Stable Asynchrony: Variance-Controlled Off-Policy RL for LLM...](https://arxiv.org/abs/2602.17616v1) | Luke Huang, Zhuoyang Zhang et al. | 2026-02-19 | cs.LG, cs.AI |
| [Towards Anytime-Valid Statistical Watermarking](https://arxiv.org/abs/2602.17608v1) | Baihe Huang, Eric Xu et al. | 2026-02-19 | cs.LG, cs.AI |
| [ODESteer: A Unified ODE-Based Steering Framework for LLM Ali...](https://arxiv.org/abs/2602.17560v1) | Hongjue Zhao, Haosen Sun et al. | 2026-02-19 | cs.AI |
| [RetouchIQ: MLLM Agents for Instruction-Based Image Retouchin...](https://arxiv.org/abs/2602.17558v1) | Qiucheng Wu, Jing Shi et al. | 2026-02-19 | cs.CV |
| [GraphThinker: Reinforcing Video Reasoning with Event Graph T...](https://arxiv.org/abs/2602.17555v1) | Zixu Cheng, Da Li et al. | 2026-02-19 | cs.CV |
| [A Theoretical Framework for Modular Learning of Robust Gener...](https://arxiv.org/abs/2602.17554v1) | Corinna Cortes, Mehryar Mohri et al. | 2026-02-19 | cs.LG, stat.ML |
| [MASPO: Unifying Gradient Utilization, Probability Mass, and ...](https://arxiv.org/abs/2602.17550v1) | Xiaoliang Fu, Jiaye Lin et al. | 2026-02-19 | cs.LG, cs.AI |
| [Using LLMs for Knowledge Component-level Correctness Labelin...](https://arxiv.org/abs/2602.17542v1) | Zhangqi Duan, Arnav Kankaria et al. | 2026-02-19 | cs.CL, cs.CY |
| [Enhancing Large Language Models (LLMs) for Telecom using Dyn...](https://arxiv.org/abs/2602.17529v1) | Dun Yuan, Hao Zhou et al. | 2026-02-19 | cs.AI |
| [Global Self-Attention with Exact Fourier Propagation for Pha...](https://arxiv.org/abs/2602.17624v1) | Dilawer Singh, Antoni J. Wojcik et al. | 2026-02-19 | physics.optics |
| [Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dia...](https://arxiv.org/abs/2602.17469v1) | Nusrat Jahan Lia, Shubhashis Roy Dipta | 2026-02-19 | cs.CL, cs.HC |
| [Representation Collapse in Machine Translation Through the L...](https://arxiv.org/abs/2602.17287v1) | Evgeniia Tokarchuk, Maya K. Nachesa et al. | 2026-02-19 | cs.CL, cs.LG |
| [A statistical perspective on transformers for small longitud...](https://arxiv.org/abs/2602.16914v1) | Kiana Farhadyar, Maren Hackenberg et al. | 2026-02-18 | stat.ME, cs.LG |
| [TopoFlow: Physics-guided Neural Networks for high-resolution...](https://arxiv.org/abs/2602.16821v1) | Ammar Kheder, Helmi Toropainen et al. | 2026-02-18 | cs.LG |
| [RefineFormer3D: Efficient 3D Medical Image Segmentation via ...](https://arxiv.org/abs/2602.16320v1) | Kavyansh Tyagi, Vishwas Rathi et al. | 2026-02-18 | eess.IV, cs.CV |
| [Approximation Theory for Lipschitz Continuous Transformers](https://arxiv.org/abs/2602.15503v1) | Takashi Furuya, Davide Murari et al. | 2026-02-17 | cs.LG, stat.ML |
| [Bottleneck Transformer-Based Approach for Improved Automatic...](https://arxiv.org/abs/2602.15484v1) |  Amartyaveer, Murali Kadambi et al. | 2026-02-17 | eess.AS, cs.LG |
| [DeepMTL2R: A Library for Deep Multi-task Learning to Rank](https://arxiv.org/abs/2602.14519v1) | Chaosheng Dong, Peiyao Xiao et al. | 2026-02-16 | cs.LG, cs.IR |
| [Selective Synchronization Attention](https://arxiv.org/abs/2602.14445v1) | Hasi Hays | 2026-02-16 | cs.LG, cs.AI |
| [Integrated Photonic Polarization Synthesizer and Analyzer](https://arxiv.org/abs/2602.17024v1) | Carson G. Valdez, Anne R. Kroo et al. | 2026-02-19 | physics.optics |
| [Piezoelectric MEMS Phase Modulator for Silicon Nitride Platf...](https://arxiv.org/abs/2602.16577v1) | Firehun T. Dullo, Paul C. Thrane et al. | 2026-02-18 | physics.optics, physics.app-ph |
| [On-chip probabilistic inference for charged-particle trackin...](https://arxiv.org/abs/2602.15946v1) | Arghya Ranjan Das, David Jiang et al. | 2026-02-17 | physics.ins-det, hep-ex |
| [PRISM: Photonics-Informed Inverse Lithography for Manufactur...](https://arxiv.org/abs/2602.15762v1) | Hongjian Zhou, Haoyu Yang et al. | 2026-02-17 | physics.optics, cs.ET |
| [Coupled integrated photonic quantum memristors using a singl...](https://arxiv.org/abs/2602.14736v1) | Alessio Baldazzi, Roy Philip George Konnoth Ancel et al. | 2026-02-16 | quant-ph |
| [Polariton-mediated binding of anti-aligned dipolar excitons](https://arxiv.org/abs/2602.13054v1) | Haifeng Kang, Quanbing Guo et al. | 2026-02-13 | physics.optics |

---

## AI Summary

### Theme 1: Language Model Training, Alignment & Efficiency
This group focuses on improving how LLMs are trained, aligned, and made more efficient, particularly through reinforcement learning and modular architectures.

**Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs**
1.  This paper identifies that high-variance gradients from stale data in asynchronous RL training can destabilize LLM fine-tuning and proposes a method to control this variance.
2.  It matters because it could make RL training for LLMs significantly faster and more stable, accelerating the development of more capable and aligned models. Related concepts: [[Reinforcement Learning from Human Feedback (RLHF)]], [[Policy Gradient Methods]], [[Training Stability]].

**MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning**
1.  This paper proposes a new RL algorithm that dynamically adjusts trust regions and clipping based on gradient quality and token probability to improve the efficiency and robustness of LLM reasoning training.
2.  It matters as it addresses core inefficiencies in current RL methods, potentially leading to models that learn complex reasoning with less data and compute. Related concepts: [[Trust Region Policy Optimization (TRPO)]], [[Sample Efficiency]], [[Reasoning in LLMs]].

**A Theoretical Framework for Modular Learning of Robust Generative Models**
1.  This paper provides a theoretical foundation for training LLMs by combining small, specialized expert models, proving they can match monolithic model performance robustly across data mixtures.
2.  It matters because it offers a principled, potentially more efficient path to building large models, reducing reliance on massive, single training runs. Related concepts: [[Mixture of Experts (MoE)]], [[Modular AI]], [[Robust Machine Learning]].

**ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment**
1.  This paper introduces a theoretical framework using ordinary differential equations to guide the manipulation of LLM internal activations for more controlled and effective alignment.
2.  It matters because it moves activation steering from an ad-hoc technique toward a principled science, enabling finer-grained control over model behavior. Related concepts: [[Representation Engineering]], [[Activation Steering]], [[Model Interpretability]].

**Approximation Theory for Lipschitz Continuous Transformers**
1.  This paper develops a class of Transformer models with bounded Lipschitz constants by design and provides theoretical guarantees for their approximation capabilities.
2.  It matters for deploying robust and stable models in safety-critical applications where small input perturbations must not cause large, erratic output changes. Related concepts: [[Model Robustness]], [[Lipschitz Continuity]], [[Formal Guarantees]].

*Most Impactful in this Theme:* **Stable Asynchrony** and **A Theoretical Framework for Modular Learning**, as they tackle fundamental bottlenecks in training scalability and architecture.

### Theme 2: Multimodal & Video Reasoning
This group explores how to equip models with better causal and structural understanding of visual and temporal data.

**GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking**
1.  This paper enhances video reasoning in MLLMs by having them explicitly construct and reason over causal event graphs derived from video content.
2.  It matters because moving from caption-based to graph-based reasoning could significantly reduce hallucinations and improve models' understanding of cause-and-effect in dynamic scenes. Related concepts: [[Video Understanding]], [[Causal Reasoning]], [[Knowledge Graphs]].

**RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward**
1.  This paper trains a multimodal LLM agent to use professional image editing software via reinforcement learning, using a generalist reward model to judge edit quality.
2.  It matters as it demonstrates a path toward creating versatile, tool-using AI agents for complex creative and professional tasks. Related concepts: [[AI Agents]], [[Tool Use in AI]], [[Multimodal Reinforcement Learning]].

### Theme 3: Model Evaluation, Safety & Watermarking
This group focuses on techniques to audit, identify, and ensure the trustworthy deployment of AI-generated content.

**Towards Anytime-Valid Statistical Watermarking**
1.  This paper develops a new watermarking method for LLM outputs using e-values, allowing for valid detection at any point in the generated text without pre-defining its length.
2.  It matters because it solves a key practical limitation of current watermarks, making it more flexible and statistically sound for identifying AI-generated text. Related concepts: [[AI Watermarking]], [[Statistical Hypothesis Testing]], [[Content Provenance]].

**Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers**
1.  This paper audits transformer models for cross-lingual sentiment alignment, finding significant safety and representational failures, especially for low-resource languages like Bengali.
2.  It matters as it highlights critical gaps in global AI safety and fairness, showing that alignment can break down across language barriers. Related concepts: [[AI Alignment]], [[AI Safety]], [[Cross-lingual Transfer]].

**Using LLMs for Knowledge Component-level Correctness Labeling in Open-ended Coding Problems**
1.  This paper uses LLMs to automatically generate fine-grained skill mastery labels (knowledge components) for student code, overcoming a major data bottleneck in educational analytics.
2.  It matters because it enables more precise student modeling and personalized learning at scale in programming education. Related concepts: [[AI for Education]], [[Automated Assessment]], [[Knowledge Tracing]].

**Bottleneck Transformer-Based Approach for Improved Automatic STOI Score Prediction**
1.  This paper uses a bottleneck transformer architecture to predict speech intelligibility scores without needing a clean reference signal.
2.  It matters for real-world speech processing applications (e.g., hearing aids, teleconferencing) where reference audio is unavailable. Related concepts: [[Speech Assessment]], [[Non-Intrusive Metrics]], [[Audio Signal Processing]].

### Theme 4: Foundational Model Mechanics & Efficiency
This group dives into the core components of transformer architectures, seeking to understand or improve their fundamental operations.

**Representation Collapse in Machine Translation Through the Lens of Angular Dispersion**
1.  This paper investigates how standard training leads to "representation collapse" in deeper Transformer layers, where embeddings occupy a narrow cone in vector space, and analyzes its impact.
2.  It matters because it diagnoses a potential inefficiency in how models use their representational capacity, which could inform better architectural designs or training schemes. Related concepts: [[Representation Learning]], [[Transformer Architecture]], [[Model Capacity]].

**Selective Synchronization Attention**
1.  This paper proposes a novel attention mechanism inspired by the synchronization of coupled oscillators (Kuramoto model) as a biologically-plausible, computationally efficient alternative to dot-product self-attention.
2.  It matters as it challenges the standard attention paradigm, potentially leading to more efficient and interpretable architectures with connections to neuroscience. Related concepts: [[Attention Mechanism]], [[Computational Neuroscience]], [[Model Efficiency]].

**What Language is This? Ask Your Tokenizer**
1.  This paper introduces a simple language identification method that uses statistics from a model's tokenizer, showing strong performance on low-resource and closely-related languages.
2.  It matters because it provides a lightweight, model-intrinsic tool for a crucial step in multilingual NLP pipelines, especially where traditional methods fail. Related concepts: [[Language Identification]], [[Tokenization]], [[Low-Resource NLP]].

### Theme 5: Specialized Applications & Domain Adaptation
This group applies advanced ML and transformer techniques to solve complex problems in specific scientific and engineering domains.

**Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation**
1.  This paper integrates dynamic knowledge graphs with explainable RAG to ground LLMs in the complex, evolving domain of telecommunications, reducing hallucinations.
2.  It matters as a blueprint for deploying reliable, domain-specific LLMs in technical industries with specialized knowledge. Related concepts: [[Retrieval-Augmented Generation (RAG)]], [[Domain Adaptation]], [[Explainable AI (XAI)]].

**TopoFlow: Physics-guided Neural Networks for high-resolution air quality prediction**
1.  This paper creates a neural network that explicitly incorporates topography and wind physics to improve the accuracy and efficiency of high-resolution air pollution forecasting.
2.  It matters for environmental monitoring and public health, offering a scalable, physics-informed model for a critical prediction task. Related concepts: [[Physics-Informed Machine Learning]], [[Spatiotemporal Forecasting]], [[Environmental AI]].

**RefineFormer3D: Efficient 3D Medical Image Segmentation via Adaptive Multi-Scale Transformer with Cross Attention Fusion**
1.  This paper proposes a lightweight 3D transformer architecture that balances accuracy and efficiency for segmenting volumetric medical images like MRIs and CT scans.
2.  It matters for clinical adoption, where computational constraints are real, by making powerful transformer models feasible for medical diagnostics. Related concepts: [[Medical Image Segmentation]], [[Efficient Transformers]], [[3D Computer Vision]].

**A statistical perspective on transformers for small longitudinal cohort data**
1.  This paper explores the use of transformer models to analyze longitudinal medical/cohort data, where the importance of past observations varies, and dataset sizes are often limited.
2.  It matters because it adapts a dominant architecture from NLP to a core challenge in healthcare and social science research. Related concepts: [[Longitudinal Data Analysis]], [[Time Series Forecasting]], [[Small Data Learning]].

**DeepMTL2R: A Library for Deep Multi-task Learning to Rank**
1.  This paper presents an open-source library that implements numerous multi-task learning algorithms for ranking problems, using transformers to balance multiple objectives.
2.  It matters as it provides a standardized toolkit for researchers and practitioners in search and recommendation systems to optimize complex, real-world ranking criteria. Related concepts: [[Learning to Rank]], [[Multi-Task Learning]], [[Recommender Systems]].

### Theme 6: Advanced Photonics & Hardware for AI
This group features cutting-edge research in photonic integrated circuits and optical computing, which are relevant for next-generation AI hardware.

**PRISM: Photonics-Informed Inverse Lithography for Manufacturable Inverse-Designed Photonic Integrated Circuits**
1.  This paper introduces a method to make complex, AI-designed photonic components more robust to manufacturing imperfections, bridging the gap between simulation and fabrication.
2.  It matters critically for the practical deployment of advanced photonic chips, which are promising for ultra-fast, low-energy computing. **Cross-Domain Signal:** This directly connects to VLSI/chip design, applying inverse lithography techniques—common in semiconductor manufacturing—to the emerging field of photonic integrated circuits.

**Integrated Photonic Polarization Synthesizer and Analyzer**
1.  This paper demonstrates a single photonic chip that can both generate and measure any polarization state of light, a fundamental capability for optical communications and sensing.
2.  It matters for miniaturizing and improving the performance of systems that rely on precise control of light, such as in quantum computing and LiDAR.

**Piezoelectric MEMS Phase Modulator for Silicon Nitride Platform in the Visible Spectrum**
1.  This paper shows a compact, low-power phase modulator for visible-light photonic circuits, using a piezoelectric MEMS actuator to mechanically shift a waveguide.
2.  It matters for building active, programmable photonic systems for applications like augmented reality displays and biosensing. **Cross-Domain Signal:** The use of MEMS (Micro-Electro-Mechanical Systems) actuators is a classic microelectronics technique being innovatively applied to photonics, showing convergence in hardware platforms.

**Global Self-Attention with Exact Fourier Propagation for Phase-Only Far-Field Holography**
1.  This paper frames the design of phase-only holograms as a global optimization problem analogous to attention, using Fourier transforms to model light propagation.
2.  It matters for creating high-quality holographic displays. **Cross-Domain Signal:** The mathematical formulation draws a direct analogy between optical diffraction and the self-attention mechanism in transformers, suggesting a deep conceptual link between physics and AI architecture.

**Coupled integrated photonic quantum memristors using a single photon source made of a colour center**
1.  This paper experimentally demonstrates a network of two photonic "quantum memristors" on a chip, creating history-dependent quantum dynamics.
2.  It matters for building novel quantum neuromorphic hardware that could potentially learn or process information in fundamentally new ways.

**Polariton-mediated binding of anti-aligned dipolar excitons**
1.  This paper explores how light-matter particles (polaritons) can mediate interactions between exotic excitons, enabling new quantum many-body states.
2.  It matters for fundamental quantum optics and the long-term goal of developing quantum simulators and materials.

**On-chip probabilistic inference for charged-particle tracking at the sensor edge**
1.  This paper explores running probabilistic machine learning models directly on sensor chips at particle colliders to make real-time data filtering decisions.
2.  It matters for handling extreme data rates in fundamental science. **Cross-Domain Signal:** This represents the ultimate "edge AI" for science, pushing ML inference into specialized, radiation-hardened hardware, a trend visible in discussions about real-time processing in robotics and IoT.

---

### Cross-Domain Signals Summary
*   **Chip Design/VLSI:** **PRISM**'s use of inverse lithography is a direct import from semiconductor manufacturing into photonics. **Piezoelectric MEMS Phase Modulator** and **On-chip probabilistic inference** show the integration of traditional microelectronics (MEMS, ASIC design) with photonics and ML for next-generation sensors and processors.
*   **Philosophy of Mind / Neuroscience:** **Selective Synchronization Attention** explicitly grounds a core AI mechanism in models of biological neural synchronization (Kuramoto model), proposing a bridge between artificial and natural intelligence.
*   **Industry Trends:** The focus on **modular learning**, **training efficiency (Stable Asynchrony)**, and **manufacturability (PRISM)** reflects broader industry pushes towards cheaper, more scalable, and deployable AI. **On-chip inference** for particle physics aligns with the pervasive "edge AI" trend discussed in tech forums for robotics, autonomous vehicles, and IoT.

---

*Searched topics: large language model, transformer architecture, VLSI design, integrated circuit, philosophy of mind*
